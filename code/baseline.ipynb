{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for NER on Video Comments\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Load video comment data from a CSV file.\n",
    "2. Preprocess the data and align token-level labels.\n",
    "3. Fine-tune a pre-trained BERT model (using Hugging Face Transformers) for Named Entity Recognition.\n",
    "4. Evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: BERT-NER\n",
    "\n",
    "This notebook implements a baseline Named Entity Recognition (NER) model using a fine-tuned BERT architecture. It serves as the foundation for evaluating improvements introduced by contextual embeddings and clustering methods in our final pipeline.\n",
    "\n",
    "We evaluate the model using standard NER metrics (precision, recall, F1-score) on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: seqeval in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/anaconda3/lib/python3.11/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.11/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers seqeval torch \"accelerate>=0.26.0\"\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "if not hasattr(torch, \"get_default_device\"):\n",
    "    torch.get_default_device = lambda: torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification,\n",
    "                          TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import ast\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Likes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Replies",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Comment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Relevance",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Polarity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Feature request",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Problem report",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Efficiency",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Safety",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "labels",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "has_entity",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "entity_tokens",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "combined_labels_str",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "548af8a6-7a8e-4475-9d02-856f27e6efff",
       "rows": [
        [
         "0",
         "UghhPYDEB6B173gCoAEC",
         "2017-04-28T18:12:45Z",
         "Aaron Brown",
         "1679",
         "30",
         "i want what he's smoking",
         "spam",
         "neutral",
         "False",
         "False",
         "False",
         "false",
         "['i', 'want', 'what', 'he', \"'s\", 'smoking']",
         "['spam', 'neutral', False, False, False, 'false']",
         "6",
         "True",
         "['i', 'want', 'what', 'he', \"'s\", 'smoking']",
         "False, False, False, false, neutral, spam"
        ],
        [
         "1",
         "Ugh6WAPQinruAHgCoAEC",
         "2017-04-28T18:15:14Z",
         "Felician Cadar",
         "684",
         "22",
         "I love how Musk always makes seemingly wild claims that turn out to be reasonable ideas, conclusions of first principles reasoning.",
         "spam",
         "positive",
         "False",
         "False",
         "False",
         "false",
         "['I', 'love', 'how', 'Musk', 'always', 'makes', 'seemingly', 'wild', 'claims', 'that', 'turn', 'out', 'to', 'be', 'reasonable', 'ideas', ',', 'conclusions', 'of', 'first', 'principles', 'reasoning', '.']",
         "['spam', 'positive', False, False, False, 'false']",
         "23",
         "True",
         "['I', 'love', 'how', 'Musk', 'always', 'makes']",
         "False, False, False, false, positive, spam"
        ],
        [
         "2",
         "Ugj9xobHmVeDEHgCoAEC",
         "2017-04-28T18:24:53Z",
         "Kelvin Yang",
         "0",
         "0",
         "No.3",
         "spam",
         "neutral",
         "False",
         "False",
         "False",
         "false",
         "['No.3']",
         "['spam', 'neutral', False, False, False, 'false']",
         "1",
         "True",
         "['No.3']",
         "False, False, False, false, neutral, spam"
        ],
        [
         "3",
         "Ugj39PRg5dVn8XgCoAEC",
         "2017-04-28T18:25:31Z",
         "Kelvin Yang",
         "140",
         "4",
         "Could be the start of a historical company",
         "spam",
         "neutral",
         "False",
         "False",
         "False",
         "false",
         "['Could', 'be', 'the', 'start', 'of', 'a', 'historical', 'company']",
         "['spam', 'neutral', False, False, False, 'false']",
         "8",
         "True",
         "['Could', 'be', 'the', 'start', 'of', 'a']",
         "False, False, False, false, neutral, spam"
        ],
        [
         "4",
         "Ugiu9jMmiWts1HgCoAEC",
         "2017-04-28T18:31:52Z",
         "serendipity42",
         "675",
         "9",
         "Gotta start somewhere before making tunnels on Mars",
         "spam",
         "neutral",
         "False",
         "False",
         "False",
         "false",
         "['Got', 'ta', 'start', 'somewhere', 'before', 'making', 'tunnels', 'on', 'Mars']",
         "['spam', 'neutral', False, False, False, 'false']",
         "9",
         "True",
         "['Got', 'ta', 'start', 'somewhere', 'before', 'making']",
         "False, False, False, false, neutral, spam"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Feature request</th>\n",
       "      <th>Problem report</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>Safety</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>has_entity</th>\n",
       "      <th>entity_tokens</th>\n",
       "      <th>combined_labels_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UghhPYDEB6B173gCoAEC</td>\n",
       "      <td>2017-04-28T18:12:45Z</td>\n",
       "      <td>Aaron Brown</td>\n",
       "      <td>1679</td>\n",
       "      <td>30</td>\n",
       "      <td>i want what he's smoking</td>\n",
       "      <td>spam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>false</td>\n",
       "      <td>['i', 'want', 'what', 'he', \"'s\", 'smoking']</td>\n",
       "      <td>['spam', 'neutral', False, False, False, 'false']</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>['i', 'want', 'what', 'he', \"'s\", 'smoking']</td>\n",
       "      <td>False, False, False, false, neutral, spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugh6WAPQinruAHgCoAEC</td>\n",
       "      <td>2017-04-28T18:15:14Z</td>\n",
       "      <td>Felician Cadar</td>\n",
       "      <td>684</td>\n",
       "      <td>22</td>\n",
       "      <td>I love how Musk always makes seemingly wild cl...</td>\n",
       "      <td>spam</td>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>false</td>\n",
       "      <td>['I', 'love', 'how', 'Musk', 'always', 'makes'...</td>\n",
       "      <td>['spam', 'positive', False, False, False, 'fal...</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>['I', 'love', 'how', 'Musk', 'always', 'makes']</td>\n",
       "      <td>False, False, False, false, positive, spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugj9xobHmVeDEHgCoAEC</td>\n",
       "      <td>2017-04-28T18:24:53Z</td>\n",
       "      <td>Kelvin Yang</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No.3</td>\n",
       "      <td>spam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>false</td>\n",
       "      <td>['No.3']</td>\n",
       "      <td>['spam', 'neutral', False, False, False, 'false']</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>['No.3']</td>\n",
       "      <td>False, False, False, false, neutral, spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugj39PRg5dVn8XgCoAEC</td>\n",
       "      <td>2017-04-28T18:25:31Z</td>\n",
       "      <td>Kelvin Yang</td>\n",
       "      <td>140</td>\n",
       "      <td>4</td>\n",
       "      <td>Could be the start of a historical company</td>\n",
       "      <td>spam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>false</td>\n",
       "      <td>['Could', 'be', 'the', 'start', 'of', 'a', 'hi...</td>\n",
       "      <td>['spam', 'neutral', False, False, False, 'false']</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>['Could', 'be', 'the', 'start', 'of', 'a']</td>\n",
       "      <td>False, False, False, false, neutral, spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugiu9jMmiWts1HgCoAEC</td>\n",
       "      <td>2017-04-28T18:31:52Z</td>\n",
       "      <td>serendipity42</td>\n",
       "      <td>675</td>\n",
       "      <td>9</td>\n",
       "      <td>Gotta start somewhere before making tunnels on...</td>\n",
       "      <td>spam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>false</td>\n",
       "      <td>['Got', 'ta', 'start', 'somewhere', 'before', ...</td>\n",
       "      <td>['spam', 'neutral', False, False, False, 'false']</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>['Got', 'ta', 'start', 'somewhere', 'before', ...</td>\n",
       "      <td>False, False, False, false, neutral, spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID                  Date          Author  Likes  Replies  \\\n",
       "0  UghhPYDEB6B173gCoAEC  2017-04-28T18:12:45Z     Aaron Brown   1679       30   \n",
       "1  Ugh6WAPQinruAHgCoAEC  2017-04-28T18:15:14Z  Felician Cadar    684       22   \n",
       "2  Ugj9xobHmVeDEHgCoAEC  2017-04-28T18:24:53Z     Kelvin Yang      0        0   \n",
       "3  Ugj39PRg5dVn8XgCoAEC  2017-04-28T18:25:31Z     Kelvin Yang    140        4   \n",
       "4  Ugiu9jMmiWts1HgCoAEC  2017-04-28T18:31:52Z   serendipity42    675        9   \n",
       "\n",
       "                                             Comment Relevance  Polarity  \\\n",
       "0                           i want what he's smoking      spam   neutral   \n",
       "1  I love how Musk always makes seemingly wild cl...      spam  positive   \n",
       "2                                               No.3      spam   neutral   \n",
       "3         Could be the start of a historical company      spam   neutral   \n",
       "4  Gotta start somewhere before making tunnels on...      spam   neutral   \n",
       "\n",
       "   Feature request  Problem report  Efficiency Safety  \\\n",
       "0            False           False       False  false   \n",
       "1            False           False       False  false   \n",
       "2            False           False       False  false   \n",
       "3            False           False       False  false   \n",
       "4            False           False       False  false   \n",
       "\n",
       "                                              tokens  \\\n",
       "0       ['i', 'want', 'what', 'he', \"'s\", 'smoking']   \n",
       "1  ['I', 'love', 'how', 'Musk', 'always', 'makes'...   \n",
       "2                                           ['No.3']   \n",
       "3  ['Could', 'be', 'the', 'start', 'of', 'a', 'hi...   \n",
       "4  ['Got', 'ta', 'start', 'somewhere', 'before', ...   \n",
       "\n",
       "                                              labels  num_tokens  has_entity  \\\n",
       "0  ['spam', 'neutral', False, False, False, 'false']           6        True   \n",
       "1  ['spam', 'positive', False, False, False, 'fal...          23        True   \n",
       "2  ['spam', 'neutral', False, False, False, 'false']           1        True   \n",
       "3  ['spam', 'neutral', False, False, False, 'false']           8        True   \n",
       "4  ['spam', 'neutral', False, False, False, 'false']           9        True   \n",
       "\n",
       "                                       entity_tokens  \\\n",
       "0       ['i', 'want', 'what', 'he', \"'s\", 'smoking']   \n",
       "1    ['I', 'love', 'how', 'Musk', 'always', 'makes']   \n",
       "2                                           ['No.3']   \n",
       "3         ['Could', 'be', 'the', 'start', 'of', 'a']   \n",
       "4  ['Got', 'ta', 'start', 'somewhere', 'before', ...   \n",
       "\n",
       "                          combined_labels_str  \n",
       "0   False, False, False, false, neutral, spam  \n",
       "1  False, False, False, false, positive, spam  \n",
       "2   False, False, False, false, neutral, spam  \n",
       "3   False, False, False, false, neutral, spam  \n",
       "4   False, False, False, false, neutral, spam  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df = pd.read_csv('../data/4698969/Dataset_updated.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4275, 18)\n",
      "Columns: Index(['ID', 'Date', 'Author', 'Likes', 'Replies', 'Comment', 'Relevance',\n",
      "       'Polarity', 'Feature request', 'Problem report', 'Efficiency', 'Safety',\n",
      "       'tokens', 'labels', 'num_tokens', 'has_entity', 'entity_tokens',\n",
      "       'combined_labels_str'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Format Assumptions\n",
    "\n",
    "For this notebook we assume:\n",
    "\n",
    "- **Comment:** contains the raw comment text.\n",
    "- **combined_labels_str:** contains a string representation of a list of token-level BIO labels (aligned to a whitespace tokenization of the comment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Label Set\n",
    "\n",
    "We scan through the dataset to extract all unique labels from the combined_labels_str column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from comments\n",
    "comment_entities = []\n",
    "for text in df['Comment'].dropna():\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        comment_entities.append((ent.text.strip(), ent.label_))\n",
    "\n",
    "entity_df = pd.DataFrame(comment_entities, columns=[\"Entity\", \"Label\"])\n",
    "\n",
    "# Expand entity coverage: PERSON, ORG, PRODUCT, GPE, EVENT, WORK_OF_ART\n",
    "included_labels = [\"PERSON\", \"ORG\", \"PRODUCT\", \"GPE\", \"EVENT\", \"WORK_OF_ART\"]\n",
    "entity_df = entity_df[entity_df[\"Label\"].isin(included_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lowercase keyword sets\n",
    "entity_keywords = {\n",
    "    \"B-PER\": set(entity_df[entity_df[\"Label\"] == \"PERSON\"][\"Entity\"].str.lower()),\n",
    "    \"B-ORG\": set(entity_df[entity_df[\"Label\"] == \"ORG\"][\"Entity\"].str.lower()),\n",
    "    \"B-PROD\": set(entity_df[entity_df[\"Label\"] == \"PRODUCT\"][\"Entity\"].str.lower()),\n",
    "    \"B-LOC\": set(entity_df[entity_df[\"Label\"] == \"GPE\"][\"Entity\"].str.lower()),\n",
    "    \"B-EVENT\": set(entity_df[entity_df[\"Label\"] == \"EVENT\"][\"Entity\"].str.lower()),\n",
    "    \"B-WORK\": set(entity_df[entity_df[\"Label\"] == \"WORK_OF_ART\"][\"Entity\"].str.lower()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function to assign BIO-style labels ---\n",
    "def clean_token(token):\n",
    "    return re.sub(r'[^\\w\\s]', '', token.lower())\n",
    "\n",
    "# Function to assign synthetic labels using these keyword sets\n",
    "def generate_synthetic_labels(tokens):\n",
    "    labels = []\n",
    "    for token in tokens:\n",
    "        clean = clean_token(token)\n",
    "        tag = \"O\"\n",
    "        for label_prefix, keywords in entity_keywords.items():\n",
    "            if clean in keywords:\n",
    "                tag = label_prefix\n",
    "                break\n",
    "        labels.append(tag)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label generation\n",
    "df[\"synthetic_labels\"] = df[\"tokens\"].apply(generate_synthetic_labels)\n",
    "\n",
    "# Save the final data for training\n",
    "texts = df[\"tokens\"].tolist()\n",
    "labels = df[\"synthetic_labels\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: [('[', 'O'), (\"'\", 'O'), ('i', 'O'), (\"'\", 'O'), (',', 'O'), (' ', 'O'), (\"'\", 'O'), ('w', 'O'), ('a', 'O'), ('n', 'O'), ('t', 'O'), (\"'\", 'O'), (',', 'O'), (' ', 'O'), (\"'\", 'O'), ('w', 'O'), ('h', 'O'), ('a', 'O'), ('t', 'O'), (\"'\", 'O'), (',', 'O'), (' ', 'O'), (\"'\", 'O'), ('h', 'O'), ('e', 'O'), (\"'\", 'O'), (',', 'O'), (' ', 'O'), ('\"', 'O'), (\"'\", 'O'), ('s', 'B-ORG'), ('\"', 'O'), (',', 'O'), (' ', 'O'), (\"'\", 'O'), ('s', 'B-ORG'), ('m', 'O'), ('o', 'O'), ('k', 'O'), ('i', 'O'), ('n', 'O'), ('g', 'O'), (\"'\", 'O'), (']', 'O')]\n",
      "Counter({'O': 592320, 'B-ORG': 19275, 'B-PROD': 5700})\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "print(\"Example:\", list(zip(texts[0], labels[0])))\n",
    "flat = [tag for seq in labels for tag in seq]\n",
    "print(Counter(flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for sequences with at least one named entity\n",
    "def contains_named_entity(seq):\n",
    "    return any(tag != \"O\" for tag in seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-rich sequences: 3845\n",
      "Entity-sparse sequences: 430\n"
     ]
    }
   ],
   "source": [
    "entity_rich = [(x, y) for x, y in zip(texts, labels) if contains_named_entity(y)]\n",
    "entity_sparse = [(x, y) for x, y in zip(texts, labels) if not contains_named_entity(y)]\n",
    "print(\"Entity-rich sequences:\", len(entity_rich))\n",
    "print(\"Entity-sparse sequences:\", len(entity_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampled entity sequences: 38450\n"
     ]
    }
   ],
   "source": [
    "# Upsample entity-rich sequences (x times)\n",
    "upsample_factor = 10  # try 3â€“10 depending on class imbalance\n",
    "entity_rich_upsampled = entity_rich * upsample_factor\n",
    "print(\"Upsampled entity sequences:\", len(entity_rich_upsampled))\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced = entity_rich_upsampled + entity_sparse\n",
    "random.shuffle(balanced)\n",
    "\n",
    "# Re-split into texts and labels\n",
    "texts, labels = zip(*balanced)\n",
    "texts, labels = list(texts), list(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom PyTorch Dataset for NER\n",
    "\n",
    "This dataset:\n",
    "- Uses the **Comment** column as the raw text.\n",
    "- Uses the **combined_labels_str** column (parsed into a list) as the token-level labels.\n",
    "- Tokenizes the text using BERT's tokenizer with `is_split_into_words=True` and aligns the provided labels with the sub-tokens.\n",
    "\n",
    "Note: The text is first split by whitespace so that the provided labels (which were created with a whitespace tokenization) align with the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, label_to_id, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_id = label_to_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx]\n",
    "        tags = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            [tokens],  # wrap in a list\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        label_ids = []\n",
    "\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(self.label_to_id[tags[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)  # mask subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        encoding[\"labels\"] = torch.tensor(label_ids)\n",
    "        return {k: v.squeeze() for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Training and Validation\n",
    "\n",
    "We'll use scikit-learnâ€™s train_test_split to separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: check if label sequence has at least one named entity\n",
    "def contains_entity(label_seq):\n",
    "    return any(tag != 'O' for tag in label_seq)\n",
    "\n",
    "# Create a stratify target based on presence of entity\n",
    "has_entity = [contains_entity(seq) for seq in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 70/15/15 Train/Validation/Test Split ===\n",
    "\n",
    "# First: stratified split for test set (15%)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.15, random_state=42, stratify=has_entity\n",
    ")\n",
    "\n",
    "# Update stratify info after removing test set\n",
    "has_entity_train = [contains_entity(seq) for seq in train_labels]\n",
    "\n",
    "# Then: split remaining into train (70%) and val (15%) with stratification\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1765, random_state=42, stratify=has_entity_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 27215, Validation size: 5833, Test size: 5832\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Set: ['B-ORG', 'B-PROD', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Flatten all label lists and get the unique labels\n",
    "unique_labels = sorted({label for label_seq in labels for label in label_seq})\n",
    "print(\"Label Set:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for the model\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with correct label mappings\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 27215\n",
      "Number of validation examples: 5833\n"
     ]
    }
   ],
   "source": [
    "# Create our custom datasets for training and validation.\n",
    "# Create dataset objects\n",
    "train_dataset = CustomNERDataset(train_texts, train_labels, tokenizer, label_to_id)\n",
    "val_dataset = CustomNERDataset(val_texts, val_labels, tokenizer, label_to_id)\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 5832\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomNERDataset(test_texts, test_labels, tokenizer, label_to_id)\n",
    "print(\"Number of test examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomNERDataset object at 0x199cc5110>\n",
      "Tokenized input keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
      "Tokens: ['[CLS]', '[', \"'\", 'im', \"'\", ',', \"'\", 'starting', \"'\", ',', \"'\", 'a', \"'\", ',', \"'\", 'music', '/', 'cover', \"'\", ',', \"'\", 'page', \"'\", ',', \"'\", ';', \"'\", ',', \"'\", 'sub', '##s', '##cribe', \"'\", ',', \"'\", 'to', \"'\", ',', \"'\", 'me', \"'\", ',', \"'\", 'so', \"'\", ',', \"'\", 'i', \"'\", ',', \"'\", 'can', \"'\", ',', \"'\", 'get', \"'\", ',', \"'\", 'it', \"'\", ',', \"'\", 'started', \"'\", ',', \"'\", '?', \"'\", ',', \"'\", 'thank', \"'\", ',', \"'\", 'you', \"'\", ',', \"'\", 'so', \"'\", ',', \"'\", 'much', \"'\", ',', \"'\", '[UNK]', \"'\", ']', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Aligned Labels: ['-100', 'O', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100', '-100']\n"
     ]
    }
   ],
   "source": [
    "# Inspect one tokenized sample from the training dataset.\n",
    "print(train_dataset)\n",
    "sample = train_dataset[0]\n",
    "print(\"Tokenized input keys:\", sample.keys())\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(sample[\"input_ids\"]))\n",
    "print(\"Aligned Labels:\", [id_to_label[l.item()] if l.item() != -100 else \"-100\" for l in sample[\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 5798082, 'B-ORG': 192750, 'B-PROD': 57000})\n",
      "Total unique labels: 3\n"
     ]
    }
   ],
   "source": [
    "flat_labels = [tag for seq in labels for tag in seq]\n",
    "print(Counter(flat_labels))\n",
    "print(f\"Total unique labels: {len(set(flat_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 26914\n",
      "Val: 5769\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", sum(contains_entity(seq) for seq in train_labels))\n",
    "print(\"Val:\", sum(contains_entity(seq) for seq in val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({'O': 4074920, 'B-ORG': 135458, 'B-PROD': 39959})\n"
     ]
    }
   ],
   "source": [
    "flat_train = [tag for seq in train_labels for tag in seq]\n",
    "print(\"Train label counts:\", Counter(flat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation label counts: Counter({'O': 852635, 'B-ORG': 28347, 'B-PROD': 8433})\n"
     ]
    }
   ],
   "source": [
    "flat_val = [tag for seq in val_labels for tag in seq]\n",
    "print(\"Validation label counts:\", Counter(flat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label counts: Counter({'O': 870527, 'B-ORG': 28945, 'B-PROD': 8608})\n"
     ]
    }
   ],
   "source": [
    "flat_test = [tag for seq in test_labels for tag in seq]\n",
    "print(\"Test label counts:\", Counter(flat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collator and Evaluation Metrics\n",
    "\n",
    "We use the Hugging Face DataCollator for token classification and define a compute_metrics function using seqeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        curr_labels = []\n",
    "        curr_preds = []\n",
    "        for pred, label in zip(pred_seq, label_seq):\n",
    "            if label != -100:\n",
    "                curr_labels.append(id_to_label[label])\n",
    "                curr_preds.append(id_to_label[pred])\n",
    "        true_labels.append(curr_labels)\n",
    "        true_predictions.append(curr_preds)\n",
    "    \n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "    # Uncomment the following line for a detailed report:\n",
    "    # print(classification_report(true_labels, true_predictions))\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments and Trainer Setup\n",
    "\n",
    "Adjust the training parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../report/bert-ner-video-comments\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='../report/logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/6ytyl_l54r34xk4zb10_hq680000gn/T/ipykernel_27084/4177837641.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10206' max='10206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10206/10206 1:07:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10206, training_loss=0.0003369575311080223, metrics={'train_runtime': 4054.5518, 'train_samples_per_second': 20.137, 'train_steps_per_second': 2.517, 'total_flos': 5333441644811520.0, 'train_loss': 0.0003369575311080223, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seqeval/metrics/v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: {'eval_loss': 2.404217411822174e-07, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 70.7854, 'eval_samples_per_second': 82.39, 'eval_steps_per_second': 10.299, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../report/bert-ner-video-comments/tokenizer-2025-04-03_08-49-34/tokenizer_config.json',\n",
       " '../report/bert-ner-video-comments/tokenizer-2025-04-03_08-49-34/special_tokens_map.json',\n",
       " '../report/bert-ner-video-comments/tokenizer-2025-04-03_08-49-34/vocab.txt',\n",
       " '../report/bert-ner-video-comments/tokenizer-2025-04-03_08-49-34/added_tokens.json',\n",
       " '../report/bert-ner-video-comments/tokenizer-2025-04-03_08-49-34/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(\"Test Metrics:\", metrics)\n",
    "# Save the model\n",
    "trainer.save_model(\"../report/bert-ner-video-comments/model-{}\".format(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))\n",
    "tokenizer.save_pretrained(\"../report/bert-ner-video-comments/tokenizer-{}\".format(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation on Test Set ===\n",
    "test_output = trainer.predict(test_dataset)\n",
    "predictions = test_output.predictions\n",
    "true_labels = test_output.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to predicted class indices\n",
    "predictions = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predictions and true labels to tag names\n",
    "predicted_tags = []\n",
    "true_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_seq, label_seq in zip(predictions, true_labels):\n",
    "    pred_labels = []\n",
    "    true_labels_cleaned = []\n",
    "    for pred, label in zip(pred_seq, label_seq):\n",
    "        if label != -100:\n",
    "            pred_labels.append(id_to_label[pred])\n",
    "            true_labels_cleaned.append(id_to_label[label])\n",
    "    if true_labels_cleaned:  # Only add non-empty sequences\n",
    "        predicted_tags.append(pred_labels)\n",
    "        true_tags.append(true_labels_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted tag distribution: Counter({'O': 5832})\n",
      "True tag distribution: Counter({'O': 5832})\n"
     ]
    }
   ],
   "source": [
    "flat_preds = [tag for seq in predicted_tags for tag in seq]\n",
    "flat_trues = [tag for seq in true_tags for tag in seq]\n",
    "\n",
    "print(\"Predicted tag distribution:\", Counter(flat_preds))\n",
    "print(\"True tag distribution:\", Counter(flat_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.00%\n",
      "Recall:    0.00%\n",
      "F1 Score:  0.00%\n"
     ]
    }
   ],
   "source": [
    "# Print individual metrics\n",
    "precision = precision_score(true_tags, predicted_tags, zero_division=0)\n",
    "recall = recall_score(true_tags, predicted_tags)\n",
    "f1 = f1_score(true_tags, predicted_tags)\n",
    "\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall:    {recall:.2%}\")\n",
    "print(f\"F1 Score:  {f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Evaluation on Test Set:\n",
      "Number of sequences: 5832\n",
      "Sample true tags: [['O'], ['O']]\n",
      "Sample predicted tags: [['O'], ['O']]\n",
      "True tags contain named entities? False\n",
      "Predicted tags contain named entities? False\n",
      "Classification Report:\n",
      "âš ï¸ No named entities found in either predictions or ground truth. Nothing to report.\n"
     ]
    }
   ],
   "source": [
    "# Print full report\n",
    "print(\"NER Evaluation on Test Set:\")\n",
    "print(\"Number of sequences:\", len(true_tags))\n",
    "print(\"Sample true tags:\", true_tags[:2])\n",
    "print(\"Sample predicted tags:\", predicted_tags[:2])\n",
    "def has_named_entities(sequences):\n",
    "    return any(any(tag.startswith(\"B-\") or tag.startswith(\"I-\") for tag in seq) for seq in sequences)\n",
    "print(\"True tags contain named entities?\", has_named_entities(true_tags))\n",
    "print(\"Predicted tags contain named entities?\", has_named_entities(predicted_tags))\n",
    "print(\"Classification Report:\")\n",
    "if has_named_entities(true_tags) and has_named_entities(predicted_tags):\n",
    "    print(classification_report(true_tags, predicted_tags))\n",
    "else:\n",
    "    print(\"âš ï¸ No named entities found in either predictions or ground truth. Nothing to report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Example\n",
    "\n",
    "Test the model on a new comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"This new update totally changed the way I see the future of tech!\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and get word ID mapping\n",
    "encoding = tokenizer(test_text, return_tensors=\"pt\", truncation=True, return_offsets_mapping=True, is_split_into_words=False)\n",
    "encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding).logits\n",
    "predictions = torch.argmax(outputs, dim=2).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to labels, ignoring special tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())\n",
    "predicted_labels = []\n",
    "for token, pred_id in zip(tokens, predictions):\n",
    "    label = id_to_label[pred_id]\n",
    "    predicted_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'This', 'new', 'update', 'totally', 'changed', 'the', 'way', 'I', 'see', 'the', 'future', 'of', 'tech', '!', '[SEP]']\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "entity_df.to_csv(\"../data/entity_df.csv\", index=False)\n",
    "predicted_df = pd.DataFrame({\"Token\": tokens, \"Predicted Label\": predicted_labels})\n",
    "predicted_df.to_csv(\"../data/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Baseline Results\n",
    "\n",
    "The baseline BERT-NER model, trained on video comment data with synthetic labels generated from SpaCy-extracted entities, initially failed to identify named entities during evaluation. This was primarily due to extreme class imbalance, with the majority of tokens labeled as \"O\" and very few examples containing entity tags like B-ORG or B-PROD.\n",
    "\n",
    "To address this, the dataset was rebalanced by upsampling entity-rich sequences, ensuring the model was sufficiently exposed to examples containing named entities. While this adjustment improved label diversity in the training set, the model still struggled to generalize â€” often defaulting to predicting \"O\" across all tokens.\n",
    "\n",
    "These results highlight the limitations of relying solely on keyword-matched synthetic labels and surface-level token embeddings. The baseline model lacks semantic understanding and performs poorly on informal references, spelling variations, or emerging entities not explicitly seen during training.\n",
    "\n",
    "To overcome these challenges, the extended pipeline integrates Sentence-BERT (S-BERT), which provides rich, context-aware embeddings for entire comment spans. Unlike token-based BERT models, S-BERT captures sentence-level semantics and enables similarity-based matching of unknown mentions to known entity clusters. This helps resolve ambiguities, group variants (e.g., \"Apple Vision Pro\", \"vision headset\", \"AVP\"), and improve recall on entities introduced outside the training data. Combined with unsupervised clustering, S-BERT enhances the modelâ€™s ability to detect emerging trends and identify novel or context-dependent entities in noisy, user-generated video comments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
